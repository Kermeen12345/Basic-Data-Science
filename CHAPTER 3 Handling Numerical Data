"""
Rescaling a feature

Use scikit-learn's MinMaxScaler to rescale a feature array
"""

import numpy as np
from sklearn import preprocessing

#create a feature
feature = np.array([
    [-500.5],
    [-100.1],
    [0],
    [100.1],
    [900.9]
])

#create scaler
minmax_scaler = preprocessing.MinMaxScaler(feature_range=(0,1))

#scale feature
scaled_feature = minmax_scaler.fit_transform(feature)

scaled_feature
                 #Output Expected

"""
Standardizing a Feature
"""

import numpy as np
from sklearn import preprocessing

#create a feature
feature = np.array([
    [-1000.1],
    [-200.2],
    [500.5],
    [600.6],
    [9000.9]
])

#create a scaler
scaler = preprocessing.StandardScaler()

#transform the feature
standardized = scaler.fit_transform(feature)

standardized
                 #Output Expected

print("Mean : {}".format(round(standardized.mean())))
print("Standard Deviation : {}".format(standardized.std()))
                 #Output Expected

"""
Using RobustScaler Method
"""

#create scaler
robust_scaler = preprocessing.RobustScaler()

#transform feature
robust_scaler.fit_transform(feature)
                 #Output Expected

"""
Normalizing Observations

Use scikit-learn's Normalizer to rescale the feature values to have unit norm (a total length of 1)
"""

import numpy as np
from sklearn.preprocessing import Normalizer

#create feature matrix
features = np.array([
    [0.5,0.5],
     [1.1,3.4],
    [1.5,20.2],
    [1.63,34.4],
    [10.9,3.3]
])

#create normalizer
normalizer = Normalizer(norm="l2")

#transform feature matrix
normalizer.transform(features)
                 #Output Expected

#transform feature matrix
features_l1_norm = Normalizer(norm="l1").transform(features)
print("Sum of the first observation's values : {}".format(features_l1_norm))
                 #Output Expected

"""
Grouping Observations Using Clustering
"""

import pandas as pd
from sklearn.datasets import make_blobs
from sklearn.cluster import KMeans

features, _ = make_blobs(n_samples=50,
                         n_features=2,
                         centers=3,
                        random_state=1)

df=pd.DataFrame(features,columns=["feature_1","feature_2"])

#make k-mean clusterer
clusterer = KMeans(3,random_state=0)

#fit clusterer
clusterer.fit(features)

#predict values
df['group']=clusterer.predict(features)

df.head()
                 #Output Expected

"""
Deleting Observations With Missing Values
"""

import numpy as np

features = np.array([
    [1.1,11.1],
    [2.2,22.2],
    [3.3,33.3],
    [np.nan,55]
])

#keep only observations that are not (denoted by ~) missing
features[~np.isnan(features).any(axis=1)]
                 #Output Expected

import pandas as pd

df = pd.DataFrame(features,columns=["feature_1","feature_2"])
df.dropna()
                 #Output Expected

"""
Imputing Missing Values
"""

!pip install sklearn

import numpy as np
from sklearn.preprocessing import StandardScaler
from sklearn.datasets import make_blobs
from sklearn.impute import SimpleImputer

#make fake data
features,_ = make_blobs(n_samples=1000,
                        n_features=2,
                        random_state=1)

#standardize the features
scaler = StandardScaler()
standardized_features = scaler.fit_transform(features)

#replace the first feature's first value with a missing value
true_value = standardized_features[0,0]
standardized_features[0,0]=np.nan

#create imputer
mean_imputer = SimpleImputer(strategy="mean")

#impute values
features_mean_imputed=mean_imputer.fit_transform(features)

#compare true and imputed values
print("True Values : {}".format(true_value))
print("Imputed Value : {}".format(features_mean_imputed[0,0]))
                 #Output Expected






#Collab link :
https://colab.research.google.com/drive/1mb03CdOOTTvO1OEvofepqqXcDhqX5jdm?usp=sharing
